# PocketLLM

PocketLLM is a powerful, cross-platform AI chat application built with Flutter that brings the power of large language models to your pocket. What makes PocketLLM unique is its ability to download and run AI models directly on your mobile device or connect to locally hosted models on your computer.
<div align="center">
  <img src="https://img.shields.io/badge/Status-Under_Development-orange?style=for-the-badge&logo=github" alt="Under Development">
  <img src="https://img.shields.io/badge/Release-Coming_Soon-blue?style=for-the-badge&logo=github" alt="Coming Soon">
</div>

<p align="center">
  <em>PocketLLM is currently in active development. A beta version will be released soon!</em>
</p>

## ‚ú® Key Features

- üì± **Mobile-First LLM Integration**
  - Download and run AI models directly on your mobile device
  - Optimized for mobile performance and battery life
  - Secure, private, and offline-capable operation

- üíª **Desktop Integration**
  - Seamless connection to locally hosted models via LLMStudio or Ollama
  - Easy model management and switching
  - Cross-device synchronization

- üöÄ **Advanced Features**
  - Real-time chat interface with markdown support
  - Token usage tracking and cost estimation
  - Chat history management with local storage
  - Dynamic model switching
  - Clean, modern UI with dark mode support
  - Responsive design for all screen sizes

## ü§ñ Supported Models

### Mobile-Compatible Models
- Optimized versions of popular open-source models
- Quantized models for efficient mobile execution
- Various size options (7B, 13B) for different device capabilities

### Desktop Integration
- Connect to locally hosted models via:
  - Ollama
  - LLMStudio
  - Custom API endpoints

## üõ†Ô∏è Technical Requirements

### Mobile
- Android 8.0 or later
- iOS 13.0 or later
- Minimum 4GB RAM recommended
- At least 2GB free storage space

### Desktop
- Windows 10/11
- macOS 10.15 or later
- Linux (major distributions)
- Ollama or LLMStudio installed (for local model hosting)

## üì• Installation

### Mobile Apps
Download from (Comming Soon):
- [F-Droid](link-to-fdroid)
- [GitHub Packages](link-to-github-packages)

### Desktop Apps
1. Download the latest release for your platform from the [releases page](link-to-releases)
2. Install Ollama or LLMStudio if you want to host models locally
3. Launch PocketLLM and connect to your local model server

### Build from Source
```bash
# Clone the repository
git clone https://github.com/yourusername/pocketllm.git
cd pocketllm

# Install dependencies
flutter pub get

# Run the app
flutter run
```

## üì± Getting Started

1. **Mobile Setup**
   - Launch PocketLLM
   - Browse available models in the Model Hub
   - Download your preferred model
   - Start chatting immediately

2. **Desktop Setup**
   - Install and configure Ollama or LLMStudio
   - Launch PocketLLM
   - Connect to your local model server
   - Select your model and begin chatting

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

## üì∏ Screenshots

<div style="display: flex; justify-content: space-between;">
    <img src="screenshots/mobile_chat.png" width="200" alt="Mobile Chat Interface">
    <img src="screenshots/model_selection.png" width="200" alt="Model Selection">
    <img src="screenshots/desktop_view.png" width="400" alt="Desktop Interface">
</div>

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Flutter team for the amazing framework
- Ollama project for local model hosting
- LLMStudio for desktop integration
- All open-source model providers

---
*Note: This project is currently in active development and open source. A beta version will be released soon! If you'd like to contribute, feel free to clone the repository and submit a pull request. Screenshots and documentation will be updated as the project progresses.*